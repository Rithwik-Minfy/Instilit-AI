{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54863efe-e446-4057-abd5-71e266d1fb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with shape: (100000, 18)\n",
      "            job_title experience_level employment_type company_size  \\\n",
      "0        Data Analyst              Mid        Contract       Medium   \n",
      "1     DevOps Engineer              Mid        Contract        Small   \n",
      "2  Research Scientist             Lead            None       Medium   \n",
      "3       Software Engr             Lead       Full-time        Large   \n",
      "4       Software Engr             Lead          Intern        Large   \n",
      "\n",
      "  company_location  remote_ratio salary_currency  years_experience  \\\n",
      "0          Germany             0             INR                13   \n",
      "1            India           100             GBP                 9   \n",
      "2          Germany             0             EUR                19   \n",
      "3            India            50             INR                 7   \n",
      "4          Germany           100             INR                10   \n",
      "\n",
      "     base_salary  bonus  stock_options   total_salary  salary_in_usd currency  \\\n",
      "0   68407.451747   1100          19325   88832.451747    1065.989421      USD   \n",
      "1   64193.117775   2194          19164   85551.117775  111216.453107      EUR   \n",
      "2  136071.842899   3206          12735  152012.842899  167214.127189      EUR   \n",
      "3  141850.905335   9594          11158  162602.905335   19512.348640      USD   \n",
      "4  121841.163226   6796            806  129443.163226    1553.317959      INR   \n",
      "\n",
      "   education  skills  conversion_rate  adjusted_total_usd  \n",
      "0        NaN     NaN            1.000        88832.451747  \n",
      "1        NaN     NaN            1.100        94106.229552  \n",
      "2        NaN     NaN            1.100       167214.127189  \n",
      "3        NaN     NaN            1.000       162602.905335  \n",
      "4        NaN     NaN            0.012         1553.317959  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "\n",
    "def load_data_from_postgres(\n",
    "    table_name,\n",
    "    db_user='postgres',\n",
    "    db_password='your_password',\n",
    "    db_host='localhost',\n",
    "    db_port='5432',\n",
    "    db_name='your_db'\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the dataset from a PostgreSQL table and validate its structure.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table in PostgreSQL.\n",
    "        db_user (str): Database username.\n",
    "        db_password (str): Database password.\n",
    "        db_host (str): Hostname of the database server.\n",
    "        db_port (str): Port number of the database server.\n",
    "        db_name (str): Name of the PostgreSQL database.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Loaded dataset.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns are missing or the dataset is empty.\n",
    "    \"\"\"\n",
    "\n",
    "    # Encode password to be URL-safe\n",
    "    encoded_password = urllib.parse.quote_plus(db_password)\n",
    "\n",
    "    # Create database connection string\n",
    "    connection_str = f'postgresql+psycopg2://{db_user}:{encoded_password}@{db_host}:{db_port}/{db_name}'\n",
    "    engine = create_engine(connection_str)\n",
    "\n",
    "    # Load data from PostgreSQL table\n",
    "    df = pd.read_sql_table(table_name, con=engine)\n",
    "\n",
    "    # Check if dataset is empty\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Dataset is empty\")\n",
    "\n",
    "    # Define required columns\n",
    "    required_columns = [\n",
    "        'job_title', 'experience_level', 'employment_type', 'company_size',\n",
    "        'company_location', 'remote_ratio', 'salary_currency', 'years_experience', 'base_salary',\n",
    "        'bonus', 'stock_options', 'total_salary', 'salary_in_usd', 'currency',\n",
    "        'conversion_rate', 'adjusted_total_usd'\n",
    "    ]\n",
    "\n",
    "    # Check for missing columns\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {', '.join(missing_columns)}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        df = load_data_from_postgres(\n",
    "            table_name='salary_data',\n",
    "            db_user='postgres',\n",
    "            db_password='1{Rithwik}',\n",
    "            db_host='localhost',\n",
    "            db_port='5432',\n",
    "            db_name='postgres'\n",
    "        )\n",
    "        print(\"Dataset loaded successfully with shape:\", df.shape)\n",
    "        print(df.head())\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42f4ec52-98ee-4f5d-9a93-58528988b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " experience_level mode used: Mid\n",
      " employment_type mode used: Part-time\n"
     ]
    }
   ],
   "source": [
    "def clean_data(df):\n",
    "    # Clean salary columns by removing leading single quotes and converting to numeric\n",
    "    salary_columns = ['base_salary', 'total_salary', 'salary_in_usd', 'adjusted_total_usd']\n",
    "    for col in salary_columns:\n",
    "        try:\n",
    "            # Remove leading single quote and convert to float\n",
    "            df[col] = df[col].astype(str).str.lstrip(\"'\").astype(float)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Invalid values in {col}: unable to convert to numeric after removing single quotes. Error: {e}\")\n",
    "        \n",
    "    # Remove education and skills columns if they exist\n",
    "    df = df.drop(columns=['education', 'skills'], errors='ignore')\n",
    "\n",
    "    # many duplicate rows found so dropping\n",
    "    df.drop_duplicates()\n",
    "\n",
    "    # Compute modes\n",
    "    exp_mode = df['experience_level'].mode().iloc[0]\n",
    "    emp_mode = df['employment_type'].mode().iloc[0]\n",
    "\n",
    "    # Fill missing/unknown values with mode\n",
    "    df['experience_level'].fillna(exp_mode, inplace=True)\n",
    "    df['employment_type'].fillna(emp_mode, inplace=True)\n",
    "\n",
    "    print(f\" experience_level mode used: {exp_mode}\")\n",
    "    print(f\" employment_type mode used: {emp_mode}\")\n",
    "\n",
    "\n",
    "    # Mapping inconsistent job titles to standard ones\n",
    "    job_title_mapping = {\n",
    "        'Software Engr': 'Software Engineer',\n",
    "        'Sofware Engneer': 'Software Engineer',\n",
    "        'Softwre Engineer': 'Software Engineer',\n",
    "        \n",
    "        'Data Scienist': 'Data Scientist',\n",
    "        'Data Scntist': 'Data Scientist',\n",
    "        'Dt Scientist': 'Data Scientist',\n",
    "        \n",
    "        'ML Engr': 'Machine Learning Engineer',\n",
    "        'Machine Learning Engr': 'Machine Learning Engineer',\n",
    "        'ML Enginer': 'Machine Learning Engineer',\n",
    "        'ML Engineer': 'Machine Learning Engineer'\n",
    "    }\n",
    "\n",
    "    # Apply the mapping\n",
    "    df['job_title'] = df['job_title'].replace(job_title_mapping)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_cleaned = clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f15ba031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Preprocessing completed and saved:\n",
      " Preprocessor: pkl_joblib_files/preprocessor.pkl\n",
      " Yeo-Johnson for y: pkl_joblib_files/yeojohnson_target_transformer.pkl\n",
      " X_train shape: (80000, 18)\n",
      " y_train (transformed) shape: (80000,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Minfy\\anaconda3\\envs\\ins-env\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy.stats.mstats import winsorize\n",
    "\n",
    "# Winsorization utility\n",
    "def winsorize_columns(df, columns, limits=(0.01, 0.01)):\n",
    "    for col in columns:\n",
    "        try:\n",
    "            df[col] = winsorize(df[col], limits=limits)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not winsorize column '{col}': {e}\")\n",
    "    return df\n",
    "\n",
    "# Custom Yeo-Johnson transformer for target\n",
    "class YeoJohnsonTargetTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "    def fit(self, y):\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        self.pt.fit(y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        return self.pt.transform(y).flatten()\n",
    "\n",
    "    def inverse_transform(self, y_transformed):\n",
    "        y_transformed = np.array(y_transformed).reshape(-1, 1)\n",
    "        return self.pt.inverse_transform(y_transformed).flatten()\n",
    "\n",
    "    def save(self, path):\n",
    "        joblib.dump(self.pt, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.pt = joblib.load(path)\n",
    "\n",
    "# Main preprocessing function\n",
    "def preprocess_data(df, save_dir=\"pkl_joblib_files\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    target_col = 'adjusted_total_usd'\n",
    "    numeric_cols = ['years_experience', 'base_salary', 'bonus', 'stock_options', 'total_salary', 'salary_in_usd']\n",
    "    categorical_cols = ['salary_currency', 'currency']\n",
    "    ordinal_cols = ['experience_level', 'company_size']\n",
    "    ordinal_map = [\n",
    "        ['Junior', 'Mid', 'Senior', 'Lead'],\n",
    "        ['Small', 'Medium', 'Large']\n",
    "    ]\n",
    "\n",
    "    # Step 1: Split data\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Step 2: Winsorize\n",
    "    X_train = winsorize_columns(X_train.copy(), numeric_cols)\n",
    "    X_test = winsorize_columns(X_test.copy(), numeric_cols)\n",
    "\n",
    "    # Step 3: Target transformation (Yeo-Johnson)\n",
    "    y_transformer = YeoJohnsonTargetTransformer()\n",
    "    y_transformer.fit(y_train)\n",
    "    y_train_trans = y_transformer.transform(y_train)\n",
    "    y_test_trans = y_transformer.transform(y_test)\n",
    "\n",
    "    # Save the target transformer\n",
    "    y_transformer.save(os.path.join(save_dir, \"yeojohnson_target_transformer.pkl\"))\n",
    "\n",
    "    # Step 4: Column setup\n",
    "    ordinal_features = [col for col in ordinal_cols if col in X.columns]\n",
    "    ordinal_ordering = [ordering for col, ordering in zip(ordinal_cols, ordinal_map) if col in X.columns]\n",
    "    nominal_features = [col for col in categorical_cols if col not in ordinal_features]\n",
    "\n",
    "    # Step 5: Build transformers\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('yeojohnson', PowerTransformer(method='yeo-johnson')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    ordinal_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('ordinal', OrdinalEncoder(categories=ordinal_ordering))\n",
    "    ])\n",
    "\n",
    "    nominal_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "\n",
    "    # Step 6: Combine all\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('ord', ordinal_transformer, ordinal_features),\n",
    "        ('nom', nominal_transformer, nominal_features)\n",
    "    ])\n",
    "\n",
    "    # Step 7: Transform X\n",
    "    X_train_trans = preprocessor.fit_transform(X_train)\n",
    "    X_test_trans = preprocessor.transform(X_test)\n",
    "\n",
    "    #  Save the full preprocessor\n",
    "    joblib.dump(preprocessor, os.path.join(save_dir, \"preprocessor.pkl\"))\n",
    "\n",
    "    # Step 8: Rebuild DataFrames\n",
    "    encoded_nominal_cols = preprocessor.named_transformers_['nom']['onehot'].get_feature_names_out(nominal_features)\n",
    "    feature_names = numeric_cols + ordinal_features + list(encoded_nominal_cols)\n",
    "\n",
    "    X_train_df = pd.DataFrame(X_train_trans, columns=feature_names, index=X_train.index)\n",
    "    X_test_df = pd.DataFrame(X_test_trans, columns=feature_names, index=X_test.index)\n",
    "\n",
    "    print(\" Preprocessing completed and saved:\")\n",
    "    print(f\" Preprocessor: {save_dir}/preprocessor.pkl\")\n",
    "    print(f\" Yeo-Johnson for y: {save_dir}/yeojohnson_target_transformer.pkl\")\n",
    "    print(\" X_train shape:\", X_train_df.shape)\n",
    "    print(\" y_train (transformed) shape:\", y_train_trans.shape)\n",
    "\n",
    "    return X_train_df, X_test_df, y_train_trans, y_test_trans, y_transformer\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    X_train, X_test, y_train, y_test, y_transformer = preprocess_data(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfe1f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ridge...\n",
      "ridge best RMSE: 0.2400 | Best Params: {'alpha': 1.0}\n",
      "Training lasso...\n",
      "lasso best RMSE: 0.2404 | Best Params: {'alpha': 0.001}\n",
      "Training random_forest...\n",
      "random_forest best RMSE: 0.0809 | Best Params: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Training xgboost...\n",
      "xgboost best RMSE: 0.0831 | Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}\n",
      "Training lightgbm...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1324\n",
      "[LightGBM] [Info] Number of data points in the train set: 80000, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "lightgbm best RMSE: 0.0800 | Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}\n",
      "\n",
      "Best Model: lightgbm\n",
      "Test RMSE: 0.6375\n",
      "Test RÂ² Score: 0.9452\n",
      "Model saved to: pkl_joblib_files\\model.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def train_evaluate_and_select_model(X_train, y_train, X_test, y_test, save_dir=\"pkl_joblib_files\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Model grid\n",
    "    models = {\n",
    "        'ridge': {\n",
    "            'model': Ridge(),\n",
    "            'params': {'alpha': [0.1, 1.0, 10.0]}\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'params': {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'model': XGBRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'model': LGBMRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [-1, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    best_name = None\n",
    "\n",
    "    for name, config in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "\n",
    "        grid = GridSearchCV(config['model'], config['params'],\n",
    "                            cv=5, scoring='neg_root_mean_squared_error',\n",
    "                            n_jobs=-1, verbose=0)\n",
    "        grid.fit(X_train, y_train)\n",
    "\n",
    "        rmse = -grid.best_score_\n",
    "        print(f\"{name} best RMSE: {rmse:.4f} | Best Params: {grid.best_params_}\")\n",
    "\n",
    "        if rmse < best_score:\n",
    "            best_score = rmse\n",
    "            best_model = grid.best_estimator_\n",
    "            best_name = name\n",
    "\n",
    "    # Evaluate on test set\n",
    "    y_pred_test = best_model.predict(X_test)\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "    print(f\"\\nBest Model: {best_name}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"Test RÂ² Score: {test_r2:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    model_path = os.path.join(save_dir, \"model.pkl\")\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "    return best_model, y_pred_test\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    best_model, y_pred_test = train_evaluate_and_select_model(X_train, y_train, X_test, y_test, save_dir=\"pkl_joblib_files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b3ec0c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train shape: (80000, 18), saved to data\\train.csv\n",
      "âœ… Test shape: (20000, 18), saved to data\\test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def save_train_test_for_evidently(df, target_column=\"adjusted_total_usd\", test_size=0.2, random_state=42, output_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into train/test and saves both to CSV including X and y.\n",
    "    This is used for Evidently drift detection.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Full dataset with features + target column.\n",
    "        target_column (str): Name of the target column.\n",
    "        test_size (float): Fraction of test data.\n",
    "        random_state (int): Seed for reproducibility.\n",
    "        output_dir (str): Folder where train/test files are saved.\n",
    "    \"\"\"\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in DataFrame.\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Split\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Save\n",
    "    train_path = os.path.join(output_dir, \"train.csv\")\n",
    "    test_path = os.path.join(output_dir, \"test.csv\")\n",
    "\n",
    "    train_df.to_csv(train_path, index=False)\n",
    "    test_df.to_csv(test_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Train shape: {train_df.shape}, saved to {train_path}\")\n",
    "    print(f\"âœ… Test shape: {test_df.shape}, saved to {test_path}\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    save_train_test_for_evidently(df, target_column=\"adjusted_total_usd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63078738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training ridge...\n",
      "ridge | MAE: 22.24 | RMSE: 33.89 | R2: -153.979 | MAPE: 11.75% | Best Params: {'alpha': 1.0}\n",
      "Saved ridge model to saved_models/ridge_best.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  3.22it/s]\n",
      "2025/07/07 07:24:11 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run ridge at: http://localhost:5000/#/experiments/576117582094812384/runs/7c3e6bd243ff409e993a16067a105dca.\n",
      "2025/07/07 07:24:11 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/576117582094812384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SHAP saved & logged: shap_outputs\\ridge_shap_summary.png\n",
      "\n",
      "Training lasso...\n",
      "lasso | MAE: 21.35 | RMSE: 32.53 | R2: -141.861 | MAPE: 11.32% | Best Params: {'alpha': 0.001}\n",
      "Saved lasso model to saved_models/lasso_best.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  3.28it/s]\n",
      "2025/07/07 07:24:28 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run lasso at: http://localhost:5000/#/experiments/576117582094812384/runs/54105ef40c4b49259ce378f83c7bcbf6.\n",
      "2025/07/07 07:24:28 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/576117582094812384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SHAP saved & logged: shap_outputs\\lasso_shap_summary.png\n",
      "\n",
      "Training random_forest...\n",
      "random_forest | MAE: 0.35 | RMSE: 0.65 | R2: 0.942 | MAPE: 0.10% | Best Params: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Saved random_forest model to saved_models/random_forest_best.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:08<00:00,  1.20s/it]\n",
      "100%|===================| 14974/15000 [10:38<00:01]        2025/07/07 07:43:42 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run random_forest at: http://localhost:5000/#/experiments/576117582094812384/runs/4ff4dc02c54b432c90aefc70c8da5e86.\n",
      "2025/07/07 07:43:42 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/576117582094812384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SHAP saved & logged: shap_outputs\\random_forest_shap_summary.png\n",
      "\n",
      "Training xgboost...\n",
      "xgboost | MAE: 0.36 | RMSE: 0.66 | R2: 0.942 | MAPE: 0.15% | Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}\n",
      "Saved xgboost model to saved_models/xgboost_best.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.90it/s]\n",
      " 99%|===================| 14804/15000 [01:17<00:01]        2025/07/07 07:45:21 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run xgboost at: http://localhost:5000/#/experiments/576117582094812384/runs/09fc8cffe86748478f1987fa409dc3d9.\n",
      "2025/07/07 07:45:21 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/576117582094812384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SHAP saved & logged: shap_outputs\\xgboost_shap_summary.png\n",
      "\n",
      "Training lightgbm...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1324\n",
      "[LightGBM] [Info] Number of data points in the train set: 45000, number of used features: 18\n",
      "[LightGBM] [Info] Start training from score -0.003240\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "lightgbm | MAE: 0.35 | RMSE: 0.64 | R2: 0.945 | MAPE: 0.14% | Best Params: {'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100}\n",
      "Saved lightgbm model to saved_models/lightgbm_best.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  3.11it/s]\n",
      "100%|===================| 14993/15000 [00:54<00:00]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ SHAP failed for lightgbm: Additivity check failed in TreeExplainer! Please ensure the data matrix you passed to the explainer is the same shape that the model was trained on. If your data shape is correct then please report this on GitHub. This check failed because for one of the samples the sum of the SHAP values was -0.075366, while the model output was -0.079867. If this difference is acceptable you can set check_additivity=False to disable this check.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/07 07:46:36 INFO mlflow.tracking._tracking_service.client: ðŸƒ View run lightgbm at: http://localhost:5000/#/experiments/576117582094812384/runs/94823a2bb8c84e6d83db1d94798235e9.\n",
      "2025/07/07 07:46:36 INFO mlflow.tracking._tracking_service.client: ðŸ§ª View experiment at: http://localhost:5000/#/experiments/576117582094812384.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š All Model Validation Metrics:\n",
      "        model       mae      rmse          r2      mape\n",
      "        ridge 22.239504 33.886179 -153.978895 11.754137\n",
      "        lasso 21.353578 32.534461 -141.861294 11.319156\n",
      "random_forest  0.350962  0.652971    0.942454  0.103456\n",
      "      xgboost  0.362972  0.655032    0.942090  0.151570\n",
      "     lightgbm  0.353197  0.637282    0.945186  0.141313\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# with SHAP\n",
    "def train_and_evaluate_models(models, X_train, y_train, X_test, y_test, save_dir=\"saved_models\",shap_dir=\"shap_outputs\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(shap_dir, exist_ok=True)\n",
    "\n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    mlflow.set_experiment(\"Instilit Salary Prediction\")\n",
    "\n",
    "    results = []\n",
    "    best_estimators = {}\n",
    "\n",
    "    for name, mp in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        grid = GridSearchCV(mp['model'], mp['params'], cv=3, scoring='r2', n_jobs=-1)\n",
    "        grid.fit(X_train, y_train)\n",
    "        y_pred = grid.predict(X_test)\n",
    "\n",
    "        # Metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "        metrics = {\n",
    "        \"mae\": mae,\n",
    "        \"rmse\": rmse,\n",
    "        \"r2\": r2,\n",
    "        \"mape\": mape\n",
    "    }\n",
    "\n",
    "        print(\n",
    "            f\"{name} | MAE: {mae:.2f} | RMSE: {rmse:.2f} | R2: {r2:.3f} | MAPE: {mape:.2f}% | Best Params: {grid.best_params_}\")\n",
    "\n",
    "        results.append({\n",
    "            \"model\": name,\n",
    "            \"best_params\": grid.best_params_,\n",
    "            \"mae\": mae,\n",
    "            \"rmse\": rmse,\n",
    "            \"r2\": r2,\n",
    "            \"mape\": mape\n",
    "        })\n",
    "\n",
    "        # Save the best estimator for this model\n",
    "        model_filename = f\"{save_dir}/{name}_best.pkl\"\n",
    "        joblib.dump(grid.best_estimator_, model_filename)\n",
    "        print(f\"Saved {name} model to {model_filename}\")\n",
    "\n",
    "        # Store best estimator in dictionary\n",
    "        best_estimators[name] = grid.best_estimator_\n",
    "\n",
    "        # ðŸ§ª MLflow logging\n",
    "        with mlflow.start_run(run_name=name) as run:\n",
    "            mlflow.log_params(grid.best_params_)\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "            input_example = X_test.iloc[:3] if hasattr(X_test, \"iloc\") else X_test[:3]\n",
    "            signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "            #mlflow.sklearn.log_model(grid.best_estimator_, \"model\")\n",
    "\n",
    "            mlflow.sklearn.log_model(\n",
    "                grid.best_estimator_,\n",
    "                artifact_path=\"model\",  # use 'name' instead of 'artifact_path'\n",
    "                input_example=input_example,\n",
    "                signature=signature\n",
    "                )\n",
    "\n",
    "            # âœ… SHAP Explanation\n",
    "            try:\n",
    "                explainer = shap.Explainer(grid.best_estimator_, X_val)\n",
    "                shap_values = explainer(X_val)\n",
    "\n",
    "                # Plot and save SHAP summary\n",
    "                shap_path = os.path.join(shap_dir, f\"{name}_shap_summary.png\")\n",
    "                plt.figure()\n",
    "                shap.summary_plot(shap_values, X_val, show=False)\n",
    "                plt.savefig(shap_path, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                mlflow.log_artifact(shap_path, artifact_path=\"shap_plots\")\n",
    "                print(f\"âœ… SHAP saved & logged: {shap_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ SHAP failed for {name}: {e}\")\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nðŸ“Š All Model Validation Metrics:\")\n",
    "    print(results_df[[\"model\", \"mae\", \"rmse\", \"r2\", \"mape\"]].to_string(index=False))\n",
    "\n",
    "\n",
    "    return results_df, best_estimators\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Model grid\n",
    "    models = {\n",
    "        'ridge': {\n",
    "            'model': Ridge(),\n",
    "            'params': {'alpha': [0.1, 1.0, 10.0]}\n",
    "        },\n",
    "        'lasso': {\n",
    "            'model': Lasso(),\n",
    "            'params': {'alpha': [0.001, 0.01, 0.1, 1.0]}\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'model': RandomForestRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5]\n",
    "            }\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'model': XGBRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'model': LGBMRegressor(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100],\n",
    "                'learning_rate': [0.05, 0.1],\n",
    "                'max_depth': [-1, 5]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "    train_and_evaluate_models(models, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26872f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ins-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
